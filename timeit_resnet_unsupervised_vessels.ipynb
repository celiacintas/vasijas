{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import scipy.ndimage as nd\n",
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.measure as sk\n",
    "from utils import utils\n",
    "plt.style.use('ggplot')\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tfs\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import PIL.ImageOps\n",
    "import skimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from models.classifierResnet import _C\n",
    "import visdom\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import TopKCategoricalAccuracy, Loss\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.73 s ± 244 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#### Load dataset\n",
    "\n",
    "transformations = [tfs.RandomAffine(0., scale=(0.5, 1.), fillcolor=0),\n",
    "                   tfs.Resize((128, 128)),\n",
    "                   #tfs.Grayscale(1),\n",
    "                   tfs.Lambda(lambda x: PIL.ImageOps.invert(x))]\n",
    "\n",
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose(transformations))\n",
    "\n",
    "data_loader = data.DataLoader(imagenet_data, batch_size=4, shuffle=False)\n",
    "\n",
    "#### Process type of vessels for classification\n",
    "\n",
    "df = utils.create_df_from_files(path='data/perfiles_CATA/clases/')\n",
    "\n",
    "#### Build and train classifier\n",
    "\n",
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose([tfs.RandomHorizontalFlip(p=0.7),\n",
    "                                                           tfs.RandomAffine(0, scale=(0.7, 1.), fillcolor=0),\n",
    "                                                           tfs.Resize((128, 128)),\n",
    "                                                           tfs.ToTensor()]))\n",
    "                                                           \n",
    "\n",
    "splits_len = round(len(imagenet_data.samples)*0.2), round(len(imagenet_data.samples)*0.1), round(len(imagenet_data.samples)*0.7)\n",
    "\n",
    "splits_len = splits_len if np.sum(splits_len) == len(imagenet_data.samples) else splits_len[0]-1, splits_len[1], splits_len[2] +2\n",
    "\n",
    "#### Split\n",
    "\n",
    "#All\n",
    "splits = utils.random_split(imagenet_data, splits_len)\n",
    "\n",
    "file_pi2 = open('data_pickle/indice_0.pickle', 'rb') \n",
    "splits[0].indices = pickle.load(file_pi2)\n",
    "\n",
    "file_pi2 = open('data_pickle/indice_1.pickle', 'rb') \n",
    "splits[2].indices = pickle.load(file_pi2)\n",
    "\n",
    "train_loader = data.DataLoader(splits[2], batch_size=32, shuffle=False)\n",
    "test_loader = data.DataLoader(splits[0], batch_size=32, shuffle=False)\n",
    "\n",
    "C =_C(input_h_w=128)\n",
    "C = C.to(available_device)\n",
    "\n",
    "checkpoint = torch.load(\"models/checkpoint_data_resnet/cnn_vessels_model_100.pth\")\n",
    "C.load_state_dict(checkpoint)\n",
    "\n",
    "y_real_test, X_test = utils.iterations_test_partial(C, test_loader, available_device)\n",
    "y_real_train, X_train = utils.iterations_test_partial(C, train_loader, available_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load dataset\n",
    "\n",
    "transformations = [tfs.RandomAffine(0., scale=(0.5, 1.), fillcolor=0),\n",
    "                   tfs.Resize((128, 128)),\n",
    "                   #tfs.Grayscale(1),\n",
    "                   tfs.Lambda(lambda x: PIL.ImageOps.invert(x))]\n",
    "\n",
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose(transformations))\n",
    "\n",
    "data_loader = data.DataLoader(imagenet_data, batch_size=4, shuffle=False)\n",
    "\n",
    "#### Process type of vessels for classification\n",
    "\n",
    "df = utils.create_df_from_files(path='data/perfiles_CATA/clases/')\n",
    "\n",
    "#### Build and train classifier\n",
    "\n",
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose([tfs.RandomHorizontalFlip(p=0.7),\n",
    "                                                           tfs.RandomAffine(0, scale=(0.7, 1.), fillcolor=0),\n",
    "                                                           tfs.Resize((128, 128)),\n",
    "                                                           tfs.ToTensor()]))\n",
    "                                                           \n",
    "\n",
    "splits_len = round(len(imagenet_data.samples)*0.2), round(len(imagenet_data.samples)*0.1), round(len(imagenet_data.samples)*0.7)\n",
    "\n",
    "splits_len = splits_len if np.sum(splits_len) == len(imagenet_data.samples) else splits_len[0]-1, splits_len[1], splits_len[2] +2\n",
    "\n",
    "#### Split\n",
    "\n",
    "#All\n",
    "splits = utils.random_split(imagenet_data, splits_len)\n",
    "\n",
    "file_pi2 = open('data_pickle/indice_0.pickle', 'rb') \n",
    "splits[0].indices = pickle.load(file_pi2)\n",
    "\n",
    "file_pi2 = open('data_pickle/indice_1.pickle', 'rb') \n",
    "splits[2].indices = pickle.load(file_pi2)\n",
    "\n",
    "train_loader = data.DataLoader(splits[2], batch_size=32, shuffle=False)\n",
    "test_loader = data.DataLoader(splits[0], batch_size=32, shuffle=False)\n",
    "\n",
    "C =_C(input_h_w=128)\n",
    "C = C.to(available_device)\n",
    "\n",
    "checkpoint = torch.load(\"models/checkpoint_data_resnet/cnn_vessels_model_100.pth\")\n",
    "C.load_state_dict(checkpoint)\n",
    "\n",
    "y_real_test, X_test = utils.iterations_test_partial(C, test_loader, available_device)\n",
    "y_real_train, X_train = utils.iterations_test_partial(C, train_loader, available_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 ms ± 22.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.1 ms ± 95 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500,max_depth=4)\n",
    "clf.fit(X_train, y_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 s ± 11.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clf = RandomForestClassifier(n_estimators=500,max_depth=4)\n",
    "clf.fit(X_train, y_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.4 ms ± 74 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasija",
   "language": "python",
   "name": "vasija"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
