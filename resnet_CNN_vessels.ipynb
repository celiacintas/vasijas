{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import scipy.ndimage as nd\n",
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.measure as sk\n",
    "from utils import utils\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tfs\n",
    "from torch.utils import data\n",
    "\n",
    "import torch\n",
    "import PIL.ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [tfs.RandomAffine(0., scale=(0.5, 1.), fillcolor=0),\n",
    "                   tfs.Resize((128,128)),]\n",
    "                   #tfs.Grayscale(1),\n",
    "                   #tfs.Lambda(lambda x: PIL.ImageOps.invert(x))]\n",
    "\n",
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose(transformations))\n",
    "\n",
    "data_loader = data.DataLoader(imagenet_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(imagenet_data)):\n",
    "    sample = imagenet_data[i + 10]\n",
    "    #print(sample)\n",
    "    ax = plt.subplot(1, 5, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(np.asarray(sample[0]), cmap=plt.cm.gray_r)\n",
    "    ax.set_title('Sample {}'.format(i))\n",
    "    ax.axis('off')\n",
    "\n",
    "    if i == 4:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process type of vessels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.create_df_from_files(path='data/perfiles_CATA/clases/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_data = datasets.ImageFolder('data/perfiles_CATA/png_clasificados/',\n",
    "                                     transform=tfs.Compose([tfs.RandomHorizontalFlip(p=0.7),\n",
    "                                                           tfs.RandomAffine(0, scale=(0.7, 1.), fillcolor=0),\n",
    "                                                           tfs.Resize((128, 128)),\n",
    "                                                           #tfs.Resize(128),\n",
    "                                                           #tfs.Grayscale(1),\n",
    "                                                           #tfs.Lambda(lambda x: PIL.ImageOps.invert(x)),\n",
    "                                                           tfs.ToTensor()]))\n",
    "                                                           #tfs.Normalize((0.5,), (0.5,))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_len = round(len(imagenet_data.samples)*0.2), round(len(imagenet_data.samples)*0.1), round(len(imagenet_data.samples)*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_len = splits_len if np.sum(splits_len) == len(imagenet_data.samples) else splits_len[0], splits_len[1], splits_len[2]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 128, 898)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = utils.random_split(imagenet_data, splits_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 1282\n",
       "    Root Location: data/perfiles_CATA/png_clasificados/\n",
       "    Transforms (if any): Compose(\n",
       "                             RandomHorizontalFlip(p=0.7)\n",
       "                             RandomAffine(degrees=(0, 0), scale=(0.7, 1.0))\n",
       "                             Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 559, 1248,  165,  379, 1156,  274, 1252,  380,  994,  329,  968,  991,\n",
       "         403,  103,  409,  157,  445,  538,  847,  982,  796,  372,  632,  830,\n",
       "         377,   64,  878,  473,  246,  340,  653,  684,  324,  116,  986,  816,\n",
       "        1257,  264,   67,   41,  229,  496, 1052,  315, 1194,  724, 1134,  261,\n",
       "         802,  365,  212,  339, 1048,  773,  501,  885,  516,  217,  971,  799,\n",
       "          88,  331,  899, 1132,  179,  337,  545, 1003,  408, 1009,  967,  894,\n",
       "         254,   31,  688,  671,  879,  547,  873,  421,  850, 1075,  670,  272,\n",
       "          53,  234, 1161,  752,  617,  786,  311,  783,   20,  619,  317,  975,\n",
       "         886,  457,  962, 1171,  203,  791, 1234, 1090, 1216,  812,  495,  238,\n",
       "        1092,  827,  810,  218,  593,  173,  597,  484,  540,  204,  731,    0,\n",
       "         770,  728,  639,  512,  711,  595,  576,  818,  881,  609,   86, 1060,\n",
       "        1231,  522,  127,  260, 1025, 1104,  936, 1107, 1024,  615,  488,   40,\n",
       "         205,  349,   87,  590, 1255,  206, 1229,  486,  709,   63,   35,  552,\n",
       "         485,  923,  402,  100, 1278, 1220, 1241,  954,  288, 1178,  314,  189,\n",
       "         587,  842,  914,  546,  897,  251,    6,  578,  761, 1081, 1239,  259,\n",
       "        1213,  144, 1140,  640,  985,  952,  358,  382, 1190,  748,  570,  396,\n",
       "        1187,  138,  303, 1019,  155,  359,  866,  979,  519,  477,  326, 1273,\n",
       "         698,  128, 1127, 1235,  494, 1212,  817,  305,  742, 1149,  792,  768,\n",
       "        1128, 1227, 1170,   59,  109,  800,  216,  539,  650, 1153,  667,  974,\n",
       "         156,  712,  955,  363,  837,  636,  602,  198,  171,  839,  679,  306,\n",
       "         338, 1265,  112, 1223,  498,   92,  378,  154,  433,  669,  565, 1268,\n",
       "         318,  135,  998,   39])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filehandler = open(\"data_pickle/indice_0.pickle\", 'wb') \n",
    "#pickle.dump(splits[0].indices, filehandler)\n",
    "#filehandler = open(\"data_pickle/indice_1.pickle\", 'wb') \n",
    "#pickle.dump(splits[2].indices, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pi2 = open('data_pickle/indice_0.pickle', 'rb') \n",
    "splits[0].indices = pickle.load(file_pi2)\n",
    "\n",
    "file_pi2 = open('data_pickle/indice_1.pickle', 'rb') \n",
    "splits[2].indices = pickle.load(file_pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(splits[2], batch_size=32, shuffle=True)\n",
    "val_loader = data.DataLoader(splits[1], batch_size=32, shuffle=True)\n",
    "test_loader = data.DataLoader(splits[0], batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from models.classifierResnet import _C\n",
    "import visdom\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import TopKCategoricalAccuracy, Loss\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch: 1  Avg accuracy: 0.85 Avg loss: 1.76\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.85 Avg loss: 1.85\n",
      "Training Results - Epoch: 2  Avg accuracy: 0.97 Avg loss: 1.24\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.95 Avg loss: 1.38\n",
      "Training Results - Epoch: 3  Avg accuracy: 1.00 Avg loss: 0.52\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.99 Avg loss: 0.60\n",
      "Training Results - Epoch: 4  Avg accuracy: 1.00 Avg loss: 0.46\n",
      "Validation Results - Epoch: 4  Avg accuracy: 1.00 Avg loss: 0.57\n",
      "Training Results - Epoch: 5  Avg accuracy: 0.90 Avg loss: 2.64\n",
      "Validation Results - Epoch: 5  Avg accuracy: 0.87 Avg loss: 3.18\n",
      "Training Results - Epoch: 6  Avg accuracy: 0.98 Avg loss: 0.50\n",
      "Validation Results - Epoch: 6  Avg accuracy: 0.98 Avg loss: 0.46\n",
      "Training Results - Epoch: 7  Avg accuracy: 0.99 Avg loss: 0.32\n",
      "Validation Results - Epoch: 7  Avg accuracy: 0.99 Avg loss: 0.44\n",
      "Training Results - Epoch: 8  Avg accuracy: 0.95 Avg loss: 0.92\n",
      "Validation Results - Epoch: 8  Avg accuracy: 0.95 Avg loss: 0.80\n",
      "Training Results - Epoch: 9  Avg accuracy: 1.00 Avg loss: 0.24\n",
      "Validation Results - Epoch: 9  Avg accuracy: 0.99 Avg loss: 0.34\n",
      "Training Results - Epoch: 10  Avg accuracy: 1.00 Avg loss: 0.54\n",
      "Validation Results - Epoch: 10  Avg accuracy: 1.00 Avg loss: 0.67\n",
      "Training Results - Epoch: 11  Avg accuracy: 1.00 Avg loss: 0.28\n",
      "Validation Results - Epoch: 11  Avg accuracy: 1.00 Avg loss: 0.39\n",
      "Training Results - Epoch: 12  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Validation Results - Epoch: 12  Avg accuracy: 1.00 Avg loss: 0.22\n",
      "Training Results - Epoch: 13  Avg accuracy: 1.00 Avg loss: 0.34\n",
      "Validation Results - Epoch: 13  Avg accuracy: 0.99 Avg loss: 0.52\n",
      "Training Results - Epoch: 14  Avg accuracy: 1.00 Avg loss: 0.37\n",
      "Validation Results - Epoch: 14  Avg accuracy: 1.00 Avg loss: 0.44\n",
      "Training Results - Epoch: 15  Avg accuracy: 0.99 Avg loss: 0.59\n",
      "Validation Results - Epoch: 15  Avg accuracy: 0.98 Avg loss: 0.68\n",
      "Training Results - Epoch: 16  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Validation Results - Epoch: 16  Avg accuracy: 1.00 Avg loss: 0.30\n",
      "Training Results - Epoch: 17  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Validation Results - Epoch: 17  Avg accuracy: 1.00 Avg loss: 0.10\n",
      "Training Results - Epoch: 18  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Validation Results - Epoch: 18  Avg accuracy: 1.00 Avg loss: 0.17\n",
      "Training Results - Epoch: 19  Avg accuracy: 1.00 Avg loss: 0.74\n",
      "Validation Results - Epoch: 19  Avg accuracy: 1.00 Avg loss: 0.79\n",
      "Training Results - Epoch: 20  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Validation Results - Epoch: 20  Avg accuracy: 0.99 Avg loss: 0.28\n",
      "Training Results - Epoch: 21  Avg accuracy: 1.00 Avg loss: 1.74\n",
      "Validation Results - Epoch: 21  Avg accuracy: 0.99 Avg loss: 1.83\n",
      "Training Results - Epoch: 22  Avg accuracy: 1.00 Avg loss: 0.33\n",
      "Validation Results - Epoch: 22  Avg accuracy: 1.00 Avg loss: 0.40\n",
      "Training Results - Epoch: 23  Avg accuracy: 1.00 Avg loss: 0.41\n",
      "Validation Results - Epoch: 23  Avg accuracy: 0.99 Avg loss: 0.63\n",
      "Training Results - Epoch: 24  Avg accuracy: 1.00 Avg loss: 0.10\n",
      "Validation Results - Epoch: 24  Avg accuracy: 0.99 Avg loss: 0.24\n",
      "Training Results - Epoch: 25  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 25  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Training Results - Epoch: 26  Avg accuracy: 1.00 Avg loss: 0.10\n",
      "Validation Results - Epoch: 26  Avg accuracy: 1.00 Avg loss: 0.21\n",
      "Training Results - Epoch: 27  Avg accuracy: 1.00 Avg loss: 0.85\n",
      "Validation Results - Epoch: 27  Avg accuracy: 0.99 Avg loss: 1.03\n",
      "Training Results - Epoch: 28  Avg accuracy: 1.00 Avg loss: 0.17\n",
      "Validation Results - Epoch: 28  Avg accuracy: 0.99 Avg loss: 0.19\n",
      "Training Results - Epoch: 29  Avg accuracy: 1.00 Avg loss: 0.34\n",
      "Validation Results - Epoch: 29  Avg accuracy: 0.99 Avg loss: 0.51\n",
      "Training Results - Epoch: 30  Avg accuracy: 1.00 Avg loss: 0.98\n",
      "Validation Results - Epoch: 30  Avg accuracy: 0.99 Avg loss: 0.86\n",
      "Training Results - Epoch: 31  Avg accuracy: 0.99 Avg loss: 0.62\n",
      "Validation Results - Epoch: 31  Avg accuracy: 0.98 Avg loss: 0.85\n",
      "Training Results - Epoch: 32  Avg accuracy: 1.00 Avg loss: 0.35\n",
      "Validation Results - Epoch: 32  Avg accuracy: 1.00 Avg loss: 0.42\n",
      "Training Results - Epoch: 33  Avg accuracy: 1.00 Avg loss: 0.68\n",
      "Validation Results - Epoch: 33  Avg accuracy: 1.00 Avg loss: 0.72\n",
      "Training Results - Epoch: 34  Avg accuracy: 1.00 Avg loss: 0.12\n",
      "Validation Results - Epoch: 34  Avg accuracy: 0.99 Avg loss: 0.25\n",
      "Training Results - Epoch: 35  Avg accuracy: 1.00 Avg loss: 0.09\n",
      "Validation Results - Epoch: 35  Avg accuracy: 0.99 Avg loss: 0.26\n",
      "Training Results - Epoch: 36  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 36  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Training Results - Epoch: 37  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 37  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Training Results - Epoch: 38  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Validation Results - Epoch: 38  Avg accuracy: 1.00 Avg loss: 0.24\n",
      "Training Results - Epoch: 39  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 39  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Training Results - Epoch: 40  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Validation Results - Epoch: 40  Avg accuracy: 1.00 Avg loss: 0.16\n",
      "Training Results - Epoch: 41  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Validation Results - Epoch: 41  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Training Results - Epoch: 42  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 42  Avg accuracy: 1.00 Avg loss: 0.09\n",
      "Training Results - Epoch: 43  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Validation Results - Epoch: 43  Avg accuracy: 0.99 Avg loss: 0.08\n",
      "Training Results - Epoch: 44  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Validation Results - Epoch: 44  Avg accuracy: 1.00 Avg loss: 0.28\n",
      "Training Results - Epoch: 45  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Validation Results - Epoch: 45  Avg accuracy: 0.99 Avg loss: 0.20\n",
      "Training Results - Epoch: 46  Avg accuracy: 1.00 Avg loss: 0.41\n",
      "Validation Results - Epoch: 46  Avg accuracy: 1.00 Avg loss: 0.56\n",
      "Training Results - Epoch: 47  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 47  Avg accuracy: 0.99 Avg loss: 0.14\n",
      "Training Results - Epoch: 48  Avg accuracy: 0.89 Avg loss: 3.00\n",
      "Validation Results - Epoch: 48  Avg accuracy: 0.84 Avg loss: 3.48\n",
      "Training Results - Epoch: 49  Avg accuracy: 0.99 Avg loss: 0.79\n",
      "Validation Results - Epoch: 49  Avg accuracy: 0.99 Avg loss: 0.80\n",
      "Training Results - Epoch: 50  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Validation Results - Epoch: 50  Avg accuracy: 1.00 Avg loss: 0.24\n",
      "Training Results - Epoch: 51  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 51  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Training Results - Epoch: 52  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 52  Avg accuracy: 1.00 Avg loss: 0.12\n",
      "Training Results - Epoch: 53  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 53  Avg accuracy: 0.99 Avg loss: 0.09\n",
      "Training Results - Epoch: 54  Avg accuracy: 0.99 Avg loss: 0.66\n",
      "Validation Results - Epoch: 54  Avg accuracy: 0.97 Avg loss: 0.75\n",
      "Training Results - Epoch: 55  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Validation Results - Epoch: 55  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Training Results - Epoch: 56  Avg accuracy: 0.99 Avg loss: 0.22\n",
      "Validation Results - Epoch: 56  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Training Results - Epoch: 57  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Validation Results - Epoch: 57  Avg accuracy: 1.00 Avg loss: 0.18\n",
      "Training Results - Epoch: 58  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Validation Results - Epoch: 58  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Training Results - Epoch: 59  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Validation Results - Epoch: 59  Avg accuracy: 1.00 Avg loss: 0.10\n",
      "Training Results - Epoch: 60  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 60  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Training Results - Epoch: 61  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Validation Results - Epoch: 61  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Training Results - Epoch: 62  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Validation Results - Epoch: 62  Avg accuracy: 1.00 Avg loss: 0.14\n",
      "Training Results - Epoch: 63  Avg accuracy: 1.00 Avg loss: 0.24\n",
      "Validation Results - Epoch: 63  Avg accuracy: 0.99 Avg loss: 0.70\n",
      "Training Results - Epoch: 64  Avg accuracy: 1.00 Avg loss: 0.34\n",
      "Validation Results - Epoch: 64  Avg accuracy: 0.98 Avg loss: 0.64\n",
      "Training Results - Epoch: 65  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Validation Results - Epoch: 65  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Training Results - Epoch: 66  Avg accuracy: 0.99 Avg loss: 0.89\n",
      "Validation Results - Epoch: 66  Avg accuracy: 1.00 Avg loss: 1.01\n",
      "Training Results - Epoch: 67  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Validation Results - Epoch: 67  Avg accuracy: 0.99 Avg loss: 0.13\n",
      "Training Results - Epoch: 68  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 68  Avg accuracy: 0.99 Avg loss: 0.23\n",
      "Training Results - Epoch: 69  Avg accuracy: 1.00 Avg loss: 0.01\n",
      "Validation Results - Epoch: 69  Avg accuracy: 1.00 Avg loss: 0.07\n",
      "Training Results - Epoch: 70  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 70  Avg accuracy: 1.00 Avg loss: 0.09\n",
      "Training Results - Epoch: 71  Avg accuracy: 0.99 Avg loss: 0.60\n",
      "Validation Results - Epoch: 71  Avg accuracy: 0.98 Avg loss: 0.69\n",
      "Training Results - Epoch: 72  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 72  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Training Results - Epoch: 73  Avg accuracy: 1.00 Avg loss: 0.88\n",
      "Validation Results - Epoch: 73  Avg accuracy: 0.99 Avg loss: 1.12\n",
      "Training Results - Epoch: 74  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 74  Avg accuracy: 1.00 Avg loss: 0.18\n",
      "Training Results - Epoch: 75  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 75  Avg accuracy: 1.00 Avg loss: 0.10\n",
      "Training Results - Epoch: 76  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 76  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Training Results - Epoch: 77  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 77  Avg accuracy: 1.00 Avg loss: 0.15\n",
      "Training Results - Epoch: 78  Avg accuracy: 1.00 Avg loss: 0.82\n",
      "Validation Results - Epoch: 78  Avg accuracy: 0.99 Avg loss: 0.95\n",
      "Training Results - Epoch: 79  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 79  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Training Results - Epoch: 80  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Validation Results - Epoch: 80  Avg accuracy: 1.00 Avg loss: 0.08\n",
      "Training Results - Epoch: 81  Avg accuracy: 0.99 Avg loss: 0.52\n",
      "Validation Results - Epoch: 81  Avg accuracy: 0.99 Avg loss: 0.46\n",
      "Training Results - Epoch: 82  Avg accuracy: 1.00 Avg loss: 0.20\n",
      "Validation Results - Epoch: 82  Avg accuracy: 0.98 Avg loss: 0.32\n",
      "Training Results - Epoch: 83  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 83  Avg accuracy: 0.99 Avg loss: 0.22\n",
      "Training Results - Epoch: 84  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 84  Avg accuracy: 1.00 Avg loss: 0.18\n",
      "Training Results - Epoch: 85  Avg accuracy: 1.00 Avg loss: 0.12\n",
      "Validation Results - Epoch: 85  Avg accuracy: 1.00 Avg loss: 0.41\n",
      "Training Results - Epoch: 86  Avg accuracy: 0.98 Avg loss: 1.13\n",
      "Validation Results - Epoch: 86  Avg accuracy: 0.98 Avg loss: 1.12\n",
      "Training Results - Epoch: 87  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 87  Avg accuracy: 1.00 Avg loss: 0.27\n",
      "Training Results - Epoch: 88  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 88  Avg accuracy: 1.00 Avg loss: 0.18\n",
      "Training Results - Epoch: 89  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 89  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Training Results - Epoch: 90  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 90  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Training Results - Epoch: 91  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Validation Results - Epoch: 91  Avg accuracy: 1.00 Avg loss: 0.13\n",
      "Training Results - Epoch: 92  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 92  Avg accuracy: 1.00 Avg loss: 0.12\n",
      "Training Results - Epoch: 93  Avg accuracy: 1.00 Avg loss: 0.06\n",
      "Validation Results - Epoch: 93  Avg accuracy: 1.00 Avg loss: 0.12\n",
      "Training Results - Epoch: 94  Avg accuracy: 1.00 Avg loss: 0.01\n",
      "Validation Results - Epoch: 94  Avg accuracy: 0.99 Avg loss: 0.13\n",
      "Training Results - Epoch: 95  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 95  Avg accuracy: 0.99 Avg loss: 0.14\n",
      "Training Results - Epoch: 96  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 96  Avg accuracy: 1.00 Avg loss: 0.22\n",
      "Training Results - Epoch: 97  Avg accuracy: 1.00 Avg loss: 0.02\n",
      "Validation Results - Epoch: 97  Avg accuracy: 1.00 Avg loss: 0.04\n",
      "Training Results - Epoch: 98  Avg accuracy: 0.98 Avg loss: 1.31\n",
      "Validation Results - Epoch: 98  Avg accuracy: 0.96 Avg loss: 1.64\n",
      "Training Results - Epoch: 99  Avg accuracy: 1.00 Avg loss: 0.05\n",
      "Validation Results - Epoch: 99  Avg accuracy: 0.99 Avg loss: 0.21\n",
      "Training Results - Epoch: 100  Avg accuracy: 1.00 Avg loss: 0.03\n",
      "Validation Results - Epoch: 100  Avg accuracy: 1.00 Avg loss: 0.13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f1e2178ab70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C =_C(input_h_w=128)\n",
    "C = C.to(available_device)\n",
    "criterion = nn.CrossEntropyLoss().to(available_device)\n",
    "C_optimizer = optim.Adam(C.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "vis = visdom.Visdom(port='8090')\n",
    "log_interval = 10\n",
    "trainer = create_supervised_trainer(C, C_optimizer, F.cross_entropy, device=available_device)\n",
    "\n",
    "handler_checkpoint = ModelCheckpoint('models/checkpoint_data_resnet', 'cnn_vessels', save_interval=2, n_saved=2, create_dir=True, require_empty=False, save_as_state_dict=True)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, handler_checkpoint, {'model': C, 'optimizer': C_optimizer})\n",
    "\n",
    "\n",
    "evaluator = create_supervised_evaluator(C, metrics={'accuracy': TopKCategoricalAccuracy(),\n",
    "                                                     'ce_ll': Loss(F.cross_entropy)},\n",
    "                                           device=available_device)\n",
    "\n",
    "\n",
    "train_avg_loss_window = utils.create_plot_window(vis, '#Iterations', 'Loss', \n",
    "                                                 'Training Average Loss')\n",
    "train_avg_accuracy_window = utils.create_plot_window(vis, '#Iterations', 'Accuracy',\n",
    "                                                     'Training Average Accuracy')\n",
    "val_avg_loss_window = utils.create_plot_window(vis, '#Epochs', 'Loss',\n",
    "                                               'Validation Average Loss')\n",
    "val_avg_accuracy_window = utils.create_plot_window(vis, '#Epochs', 'Accuracy',\n",
    "                                                   'Validation Average Accuracy')\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['ce_ll']\n",
    "    print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(engine.state.epoch, avg_accuracy, avg_nll))\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([avg_accuracy]),\n",
    "             win=train_avg_accuracy_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([avg_nll]),\n",
    "             win=train_avg_loss_window, update='append')\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['ce_ll']\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(engine.state.epoch, avg_accuracy, avg_nll))\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([avg_accuracy]),\n",
    "             win=val_avg_accuracy_window, update='append')\n",
    "    vis.line(X=np.array([engine.state.epoch]), Y=np.array([avg_nll]),\n",
    "             win=val_avg_loss_window, update='append')\n",
    "\n",
    "trainer.run(train_loader, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-11           [-1, 64, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "             ReLU-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
      "             ReLU-21          [-1, 128, 16, 16]               0\n",
      "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "           Conv2d-24          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
      "             ReLU-26          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-27          [-1, 128, 16, 16]               0\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "             ReLU-30          [-1, 128, 16, 16]               0\n",
      "           Conv2d-31          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 16, 16]             256\n",
      "             ReLU-33          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-34          [-1, 128, 16, 16]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "             ReLU-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "             ReLU-42            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "             ReLU-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-50            [-1, 256, 8, 8]               0\n",
      "           Conv2d-51            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-53            [-1, 512, 4, 4]               0\n",
      "           Conv2d-54            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-56            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-58            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-59            [-1, 512, 4, 4]               0\n",
      "           Conv2d-60            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-62            [-1, 512, 4, 4]               0\n",
      "           Conv2d-63            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-65            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-66            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "           ResNet-69                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 20.52\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 65.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "if available_device == \"cuda\":\n",
    "    summary(C, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "number of predictions does not match size of confusion matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c424117a3d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchnet/meter/confusionmeter.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, predicted, target)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;34m'number of predictions does not match size of confusion matrix'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: number of predictions does not match size of confusion matrix"
     ]
    }
   ],
   "source": [
    "import torchnet\n",
    "\n",
    "confusion_window = utils.create_plot_window(vis, 'True Labels', 'Predicted', 'Confusion Matrix')\n",
    "\n",
    "confusion_matrix = torchnet.meter.ConfusionMeter(11, normalized=True)\n",
    "for ii, data_ in enumerate(test_loader):\n",
    "    input_, label = data_\n",
    "    val_input = Variable(input_).to(available_device)\n",
    "    val_label = Variable(label.type(torch.LongTensor)).to(available_device)\n",
    "    score = C(val_input)\n",
    "    confusion_matrix.add(score.data.squeeze(), label.type(torch.LongTensor))\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "print(confusion_matrix.value())\n",
    "print(confusion_matrix.conf)\n",
    "vis.heatmap(confusion_matrix.value(), win=confusion_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def iterations_test(C, test_loader):\n",
    "    y_real = list()\n",
    "    y_pred = list()\n",
    "\n",
    "    for ii, data_ in enumerate(test_loader):\n",
    "        input_, label = data_\n",
    "        val_input = Variable(input_).to(available_device)\n",
    "        val_label = Variable(label.type(torch.LongTensor)).to(available_device)\n",
    "        score = C(val_input)\n",
    "        _, y_pred_batch = torch.max(score, 1)\n",
    "        y_pred_batch = y_pred_batch.cpu().squeeze().numpy()\n",
    "        y_real_batch = val_label.cpu().data.squeeze().numpy()\n",
    "        y_real.append(y_real_batch.tolist())\n",
    "        y_pred.append(y_pred_batch.tolist())\n",
    "\n",
    "    y_real = [item for batch in y_real for item in batch]\n",
    "    y_pred = [item for batch in y_pred for item in batch]\n",
    "    \n",
    "    return y_real, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        13\n",
      "           1       0.99      1.00      0.99        76\n",
      "           2       1.00      0.97      0.98        31\n",
      "           3       0.80      0.89      0.84         9\n",
      "           4       0.88      0.88      0.88        17\n",
      "           5       1.00      0.67      0.80         3\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.60      1.00      0.75         6\n",
      "           8       0.89      1.00      0.94         8\n",
      "           9       0.96      0.94      0.95        78\n",
      "          10       1.00      0.83      0.91         6\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       256\n",
      "   macro avg       0.92      0.91      0.90       256\n",
      "weighted avg       0.96      0.95      0.95       256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_real, y_pred = iterations_test(C, test_loader)\n",
    "print(metrics.classification_report(np.array(y_pred), np.array(y_real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94453125 [0.9453125, 0.94140625, 0.94921875, 0.94140625, 0.9453125]\n"
     ]
    }
   ],
   "source": [
    "avg_acc = list()\n",
    "for i in range(5):\n",
    "    y_real, y_pred = iterations_test(C, test_loader)\n",
    "    avg_acc.append(metrics.accuracy_score(np.array(y_pred), np.array(y_real)))\n",
    "print(np.mean(avg_acc), avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasija",
   "language": "python",
   "name": "vasija"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
